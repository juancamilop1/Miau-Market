import google.generativeai as genai
from django.conf import settings

# Configurar la API de Gemini
genai.configure(api_key=settings.GEMINI_API_KEY)

def get_product_recommendations(dog_type, age, size, health_conditions=None, budget=None):
    """
    Obtiene recomendaciones de productos basadas en las caracter√≠sticas del perro.
    """
    print(f"   üîç get_product_recommendations llamado con: dog_type={dog_type}, age={age}, size={size}")
    
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    # Inteligencia del prompt para recomendaciones
    prompt = f"""Eres un experto en productos para perros. Bas√°ndote en la siguiente informaci√≥n del perro, 
    proporciona recomendaciones espec√≠ficas de productos de una tienda de mascotas.

    INFORMACI√ìN DEL PERRO:
    - Raza/Tipo: {dog_type}
    - Edad: {age} a√±os
    - Tama√±o: {size}
    - Condiciones de salud especiales: {health_conditions if health_conditions else 'Ninguna'}
    - Rango de presupuesto: {budget if budget else 'No especificado'}

    Por favor, proporciona:
    1. 3-5 productos recomendados (alimento, juguetes, accesorios, etc.)
    2. Para cada producto, explica por qu√© es apropiado para este perro
    3. Incluye categor√≠as como: alimento, juguetes, cuidado, accesorios
    4. Proporciona consejos de cuidado espec√≠ficos para esta raza y edad

    Formatea la respuesta de manera clara y estructurada."""
    
    try:
        print(f"   ‚è≥ Llamando API de Gemini...")
        # Configuraci√≥n de generaci√≥n
        generation_config = {
            'temperature': 0.7,
            'max_output_tokens': 1000,  # Aumentado para evitar truncamiento
            'top_p': 0.8,
            'top_k': 40
        }
        
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        print(f"   ‚úÖ Respuesta recibida de Gemini")
        
        # Obtener el texto de la respuesta
        if hasattr(response, 'text') and response.text:
            text = response.text.strip()
            print(f"   üìù Texto: {text[:100]}...")
            if text:  # Si hay contenido, devolverlo
                return {
                    'success': True,
                    'recommendations': text,
                    'status': 'Recomendaciones generadas exitosamente'
                }
        
        # Si no hay texto, intentar obtener del contenido
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                text_parts = [part.text for part in candidate.content.parts if hasattr(part, 'text')]
                full_text = ''.join(text_parts).strip()
                if full_text:
                    print(f"   üìù Texto (de parts): {full_text[:100]}...")
                    return {
                        'success': True,
                        'recommendations': full_text,
                        'status': 'Recomendaciones generadas exitosamente'
                    }
        
        print(f"   ‚ö†Ô∏è Respuesta vac√≠a")
        return {
            'success': False,
            'error': 'La API de Gemini no pudo generar recomendaciones. Intenta de nuevo.',
            'status': 'Error: respuesta vac√≠a'
        }
    except Exception as e:
        print(f"   ‚ùå ERROR en get_product_recommendations: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'status': 'Error al generar recomendaciones'
        }


def chatbot_response(message, context=None):
    """
    Genera una respuesta conversacional del chatbot sobre cuidado de perros y productos.
    """
    print(f"   üîç chatbot_response llamado con message='{message[:50]}...'")
    
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    # Construir contexto
    context_text = ""
    if context:
        context_text = f"""
CONTEXTO DEL PERRO DEL USUARIO:
- Raza/Tipo: {context.get('dog_type', 'No especificada')}
- Edad: {context.get('age', 'No especificada')} a√±os
- Tama√±o: {context.get('size', 'No especificado')}
"""
    
    prompt = f"""Eres un experto en cuidado de perros. Responde de forma √∫til y amigable.
    
    {context_text}
    
    PREGUNTA: {message}
    
    Respuesta:"""
    
    try:
        print(f"   ‚è≥ Llamando API de Gemini para respuesta conversacional...")
        # Configuraci√≥n de generaci√≥n para chatbot
        generation_config = {
            'temperature': 0.7,
            'max_output_tokens': 1000,  # Aumentado para evitar truncamiento
            'top_p': 0.9,
            'top_k': 40
        }
        
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        print(f"   ‚úÖ Respuesta recibida de Gemini")
        
        # Obtener el texto de la respuesta
        if hasattr(response, 'text') and response.text:
            text = response.text.strip()
            print(f"   üìù Texto: {text[:100]}...")
            if text:  # Si hay contenido, devolverlo
                return {
                    'success': True,
                    'response': text,
                    'status': 'Respuesta generada exitosamente'
                }
        
        # Si no hay texto, intentar obtener del contenido
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if candidate.content and candidate.content.parts:
                text_parts = [part.text for part in candidate.content.parts if hasattr(part, 'text')]
                full_text = ''.join(text_parts).strip()
                if full_text:
                    print(f"   üìù Texto (de parts): {full_text[:100]}...")
                    return {
                        'success': True,
                        'response': full_text,
                        'status': 'Respuesta generada exitosamente'
                    }
        
        print(f"   ‚ö†Ô∏è Respuesta vac√≠a")
        return {
            'success': False,
            'error': 'La API de Gemini no pudo generar una respuesta. Intenta de nuevo.',
            'status': 'Error: respuesta vac√≠a'
        }
    except Exception as e:
        print(f"   ‚ùå ERROR en chatbot_response: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': f'Error: {str(e)}',
            'status': 'Error al generar respuesta'
        }


def generate_product_description(product_name, product_type, dog_size):
    """
    Genera descripciones de productos usando IA.
    """
    print(f"   üîç generate_product_description llamado")
    
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    prompt = f"""Eres un copywriter especializado en productos para mascotas. 
    Crea una descripci√≥n atractiva y clara para el siguiente producto:
    
    - Nombre: {product_name}
    - Tipo: {product_type}
    - Tama√±o objetivo: {dog_size}
    
    La descripci√≥n debe ser:
    - Concisa (m√°ximo 3 p√°rrafos)
    - Enfocada en beneficios
    - Incluir caracter√≠sticas principales
    - Apropiada para un sitio de compras online
    """
    
    try:
        print(f"   ‚è≥ Llamando API de Gemini...")
        # Configuraci√≥n para descripciones de productos
        generation_config = {
            'temperature': 0.6,
            'max_output_tokens': 300,
            'top_p': 0.8,
            'top_k': 30
        }
        
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
        print(f"   ‚úÖ Descripci√≥n generada")
        return {
            'success': True,
            'description': response.text,
            'status': 'Descripci√≥n generada exitosamente'
        }
    except Exception as e:
        print(f"   ‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'status': 'Error al generar descripci√≥n'
        }
